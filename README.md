# Statistical Classification of Responses Elicited by a Scenario-Based Speaking Task

Simple overview of use/purpose.

## Description

This work investigated acoustic-prosodic, linguistic, and content-related features for classifying
spoken responses elicited by a scenario-based speaking task of English as a second language. I
simplified human scores based on a scoring rubric, dividing responses into binary groups of
“unsuccessful” (N = 20) and “successful” (N = 24). I classified responses into these two groups
with 97.7% accuracy using 10 features in a logistic regression model, and 75% cross-validated
accuracy using three features in a discriminant function analysis. Each classification model
transparently covered multiple aspects of the speaking construct according to the scoring rubric,
which is backed by linguistic theory. This work serves as the first step toward building an
automated scoring engine specifically for the scenario-based assessment.

## Getting Started

### Dependencies

* Parselmouth
* Pandas
* Python 3.7

## Authors

Brady Robinson
(https://www.linkedin.com/in/brady-robinson-426055167/)


## Acknowledgements

Professor James Purpura of Teachers College, Columbia University, allowed me to use
the data for this project. The results and writing are my own and do not necessarily reflect Prof.
Purpura’s or Teachers College’s opinions. Prof. Purpura’s research team developed the SBA,
conducted the pilot study, and created the rubric. In addition, the Community Language Program
at Teachers College provided general support, especially with recruitment of participants for the
pilot study.

Inspiration, code snippets, etc.
* [readme](https://gist.github.com/DomPizzie/)
* [Parselmouth](https://parselmouth.readthedocs.io/en/stable/)
* [SBA@Columbia](https://sites.google.com/tc.columbia.edu/tc-sbla-lab)
